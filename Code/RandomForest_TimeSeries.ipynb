{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of RandomForest_TimeSeries.ipynb","version":"0.3.2","provenance":[{"file_id":"1b70NuQPkARk2c7bH_QFgB92_yjgEGDMf","timestamp":1556929392479}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"9hNeH8OeKWbi","colab_type":"code","outputId":"82ef57ff-e014-41b4-c43a-c4080d7619c3","executionInfo":{"status":"ok","timestamp":1556927024535,"user_tz":240,"elapsed":490,"user":{"displayName":"Shandy Sulen","photoUrl":"https://lh4.googleusercontent.com/-0jelZVQsMnY/AAAAAAAAAAI/AAAAAAAAAeE/E3fEIJ3ZBHk/s64/photo.jpg","userId":"14505304977573939427"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n","from sklearn.utils import shuffle\n","import librosa\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zOfavkMbgw1X","colab_type":"text"},"source":["## Random Forest/Descision Tree Parameters\n","\n","**Bootstrap**: If set to True, then bagging will use sampling with replacements (same training instance can be sampled more than once)\n","\n","**Number of Trees/Estimators**: In general, the more trees you use the better get the results. However, the improvement decreases as the number of trees increases, i.e. at a certain point the benefit in prediction performance from learning more trees will be lower than the cost in computation time for learning these additional trees. One can also perform an out-of-bag cross-validation approach and plot the results to find the optimal number of trees. If you have a large number of features, then you should also have a large number of trees so that the model isn't underfit.\n","\n","**Minimum Samples Split**: Each node in a decision tree is split based on the value of a single feature. As you go down the decision tree, the number of training samples that end up along a certain branch decreases. Splitting typically stops when the number of traning samples that end up at a node is less than the set minimum, which turns the node into a leaf node. Too large of a minimum samples split value and the tre will be too short and inaccurate. Too small of a minimum samples split value and the tree will take longer to train and overfit more to the training data.\n","\n","**Maximum Depth of Tree**: The lower the number, the less accurate your model will be.  The higher the number, the more accurate, but the more you risk overfitting. Trees in a random forest should help overcome the overfitting.\n","\n","**Maximum Features**: Refers to the random subspace method of choosing a random subset of features from the feature space. sqrt(p) is the optimal value from experience.\n","\n","**Min Samples Leaf**: Prevents hardly helpful decision branching to take place. For example, if there was a leaf node with one sample attributed to it, then that branch would be of little value.\n","\n","**Max Leaf Nodes**: Another restriction on tree growth."]},{"cell_type":"markdown","metadata":{"id":"dUbWwhVA7RWN","colab_type":"text"},"source":["## Random Forest on Prog Classification"]},{"cell_type":"code","metadata":{"id":"9UQ_xCRFN-K6","colab_type":"code","outputId":"9c250360-a7f8-4c03-c7c1-1c9f71d822ec","executionInfo":{"status":"ok","timestamp":1556927025044,"user_tz":240,"elapsed":979,"user":{"displayName":"Shandy Sulen","photoUrl":"https://lh4.googleusercontent.com/-0jelZVQsMnY/AAAAAAAAAAI/AAAAAAAAAeE/E3fEIJ3ZBHk/s64/photo.jpg","userId":"14505304977573939427"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["X = np.load('./drive/My Drive/X_V2.npy')\n","print(X.shape)\n","\n","y = np.load('./drive/My Drive/Y_V2.npy')\n","print(y.shape)\n","\n","# Shuffle and split the dataset to training and testing\n","X, y = shuffle(X, y, random_state=0)\n","X = np.reshape(X, (375,52470))\n","print(X.shape)\n","train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.10, random_state=0)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(375, 1590, 33)\n","(375, 1)\n","(375, 52470)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PDrAgZJqSRrH","colab_type":"code","outputId":"319816b1-3e9e-473c-dcc2-7ec01699a4a3","executionInfo":{"status":"ok","timestamp":1556927415823,"user_tz":240,"elapsed":77831,"user":{"displayName":"Shandy Sulen","photoUrl":"https://lh4.googleusercontent.com/-0jelZVQsMnY/AAAAAAAAAAI/AAAAAAAAAeE/E3fEIJ3ZBHk/s64/photo.jpg","userId":"14505304977573939427"}},"colab":{"base_uri":"https://localhost:8080/","height":179}},"source":["# Training\n","\n","parameters = {'bootstrap': True,\n","              'min_samples_leaf': 3,\n","              'n_estimators': 2000,\n","              'min_samples_split': 10,\n","              'max_features': 'sqrt',\n","              'max_depth': 50,\n","              'max_leaf_nodes': None}\n","\n","RF_model = RandomForestClassifier(**parameters)\n","RF_model.fit(train_X, train_y)\n","\n","RF_predictions = RF_model.predict(test_X)\n","score = accuracy_score(test_y, RF_predictions)\n","print(f\"Accuracy on 90/10 train split: {score}\")\n","print(confusion_matrix(test_y, RF_predictions))\n","\n","# Validation\n","\n","X_validation = np.load('./drive/My Drive/X_Validation_1.npy')\n","X_validation = X_validation[:,:1590,:33]\n","\n","y_validation = np.load('./drive/My Drive/Y_Validation_1.npy')\n","\n","X_validation, y_validation = shuffle(X_validation, y_validation, random_state=0)\n","X_validation = np.reshape(X_validation, (92,52470))\n","\n","RF_predictions = RF_model.predict(X_validation)\n","score = accuracy_score(y_validation, RF_predictions)\n","print(f\"Score on the Validation Set: {score}\")\n","print(confusion_matrix(y_validation, RF_predictions))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  # This is added back by InteractiveShellApp.init_path()\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy on 90/10 train split: 0.8947368421052632\n","[[34  0]\n"," [ 4  0]]\n","Score on the Validation Set: 0.7717391304347826\n","[[71  0]\n"," [21  0]]\n"],"name":"stdout"}]}]}